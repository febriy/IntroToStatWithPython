{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Remember to use the reco-pyspark environment!\n",
    "\n",
    "# Spark\n",
    "- fast real-time processing framework for big data application\n",
    "- does in-memory computations to analyse data in real time (improvement from earlier Apache Hadoop MapReduce which perform only batch processing. \n",
    "- Apache Spark can perform stream processing in real time and also batch process\n",
    "- Supports interactive queries and iterative algorithms\n",
    "- Has its own cluster manager to host its application\n",
    "- Uses HDFS (Hadoop Distributed File System) for storage\n",
    "- At a high level, every Spark application consists of a driver program that runs the user’s main function and executes various parallel operations on a cluster. \n",
    "\n",
    "# PySpark\n",
    "- Apache Spark is written in Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "- The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. \n",
    "- RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark - SparkContext\n",
    "- entry point to any spark functionality\n",
    "- tells Spark how to access a Spark cluster\n",
    "- To create a SparkContext you first need to build a SparkConf object that contains information about your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.10 |Anaconda, Inc.| (default, Mar 25 2020, 23:51:54) \n",
      "[GCC 7.3.0]\n",
      "Spark version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Spark version: {}\".format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"First App\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- appName: a name to show on the cluster UI\n",
    "- master: a Spark, Mesos or YARN cluster URL, or a special \"local\" string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)\n",
    "- fault-tolerant collection of elements that can be operated on in parallel.\n",
    "- Two ways to create RDDs:\n",
    " - parallelizing an existing collection in your driver program, or \n",
    " - referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelised Collections\n",
    "- Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once created, the distributed dataset (distData) can be operated on in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add up elements using reduce\n",
    "distData.reduce(lambda a, b: a + b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Datasets\n",
    "PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- textFile method create text file RDDs\n",
    " - Takes in file location\n",
    " - reads it as a collection of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "distFile = sc.textFile(\"README.md\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, distFile can be acted on by dataset operations. \n",
    "- add up the sizes of all lines using the map and reduce operations\n",
    " - Reduce: Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineLengths = distFile.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineLengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4456"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filter: Return a new dataset formed by selecting those elements of the source on which func returns true\n",
    "- count: Return the number of elements in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "numAs = distFile.filter(lambda s: 'a' in s).count()\n",
    "numBs = distFile.filter(lambda s: 'b' in s).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a:14, lines with b:12\n"
     ]
    }
   ],
   "source": [
    "print (\"Lines with a:{}, lines with b:{}\".format(numAs, numBs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkFiles resolves file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "path = \"test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, \"w\") as testFile:\n",
    "   _ = testFile.write(\"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_2 = SparkContext(master = \"local\", appName=\"Second App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_2.addFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(iterator):\n",
    "    with open(SparkFiles.get(\"test.txt\")) as testFile:\n",
    "        fileVal = int(testFile.readline())\n",
    "        return [x * fileVal for x in iterator]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parallelize: distribute a local Python collection to form an RDD\n",
    "- map:Return a new distributed dataset formed by passing each element of the source through a function func. \n",
    "- mapPartitions: Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator< T > => Iterator< U > when running on an RDD of type T. \n",
    "- collect(): Return all the elements of the dataset as an array at the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 200, 300, 400]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_2.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Operations\n",
    "RDDs support two types of operations: \n",
    "- transformations, which create a new dataset from an existing one, and\n",
    " - e.g. map passes dataset through a function and returns a new RDD\n",
    "- actions, which return a value to the driver program after running a computation on the dataset. \n",
    " - e.g.reduce aggregates all elements of the RDD and returns the final result to the driver program\n",
    " \n",
    "All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program.\n",
    "\n",
    "By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Function to Spark\n",
    "Spark’s API relies heavily on passing functions in the driver program to run on the cluster. There are three recommended ways to do this:\n",
    "- Lambda expressions, for simple functions that can be written as an expression. (Lambdas do not support multi-statement functions or statements that do not return a value.)\n",
    "- Local defs inside the function calling into Spark, for longer code.\n",
    "- Top-level functions in a module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more to learn\n",
    "# https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://www.tutorialspoint.com/pyspark/pyspark_quick_guide.htm\n",
    "- https://spark.apache.org/docs/latest/api/python/pyspark.html\n",
    "- https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "- https://www.youtube.com/watch?v=XrpSRCwISdk&t=20s Pyspark for Data Scientists\n",
    "- https://www.youtube.com/watch?v=5dARTeE6OpU PySpark tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reco_pyspark)",
   "language": "python",
   "name": "my_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
