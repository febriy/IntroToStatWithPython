{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "- Simple linear model\n",
    "- Generalised linear model \n",
    " - with a non-Gaussian distribution\n",
    "- Leverage and influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [statquest](https://www.youtube.com/watch?v=nk2CQITm_eo)\n",
    "\n",
    "Main ideas of linear model\n",
    "1. Use least-squares to fit a line to the data\n",
    " - compare residual from mean value ('horizontal' line) to a fitted line ('rotated' line) till min squared residual is obtained. \n",
    " - y-axes and gradient will be estimated\n",
    " - gradient != 0 means knowing x will help us guess y\n",
    " - How good is the model 'guess'?\n",
    "2. Calculate R-square\n",
    " - get average y\n",
    " - sum squared residual - SS(mean) - sum of square around the mean. Find Var(mean) = SS(mean)/n\n",
    " - sum squared residual around fit line - SS(fit)\n",
    " - Remember: in general, Var(something) = average sum of squares \n",
    "  - Var(fit) is the average SS(fit) for each datapoint\n",
    "  - Supposedly there are less variation around the fitted line. We can say that some of the variation in y is 'explained' by taking x into account\n",
    "  - R-square tells us how much of the variation in y can be explained when taking x into account\n",
    "  - R-square = (Var(mean)-Var(fit))/ Var(mean)\n",
    "  - Ideally, the more 'nonsense variable' added, nothing changes in the equation as least squares will get rid of them \n",
    "   - e.g. weight = 10+20(height)+0(favourite color)\n",
    "  - But there's a chance that 'nonsense variables' are given parameter other than 0. In this case, we might mistakenly think that the variable is actually useful\n",
    "  - so we can use adjusted R-square = R-square/number of parameters. \n",
    "3. Calculate p-value for R-square \n",
    " - imagine if we only have 2 datapoints. The fitted line will definitely have R-square=1\n",
    " - So we need a way to determine if R-square is statistically significant. We use p-value.\n",
    "\n",
    "Slight sidetrack:\n",
    "\\begin{equation*}\n",
    "R^2 = \\frac{variation\\,in\\,y\\,explained\\,by\\,x}{variation\\,in\\,y\\,without\\,taking\\,x\\,into\\,account} \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "F = \\frac{variation\\,in\\,y\\,explained\\,by\\,x}{\\,in\\,y\\,not\\,explained\\,by\\,x} \n",
    "\\end{equation*}\n",
    "\n",
    "Residual in the fitted line signifies the variation that is not explained by x \n",
    "\n",
    "\\begin{equation*}\n",
    "R^2 = \\frac{SS(mean) - SS(fit)}{SS(mean)} \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "F = \\frac{SS(mean) - SS(fit) /(p_{fit} -p_{mean})}{SS(fit) / (n-p_{fit})} \n",
    "\\end{equation*}\\\n",
    "\n",
    "p_fit is the number of parameters in the fit line\n",
    " - for simple linear, y=mx+c. Therefore, p_fit = 2, p_mean=1\n",
    " - the numerator of F is the variance explained by __the extra parameters__\n",
    " - the denominator is the variation in y not explained by the fit line\n",
    "  - why divide by n-p_fit not just n? Intuitively the more parameters you have in your equation, the more data you need to estimate them e.g. need 2 points to estimate a line and 3 points to estimate a plane.\n",
    "  - If the fit is good, the numerator will be large, denominator small. Variation is explained greatly by the extra parameters in the fit.\n",
    "  - F becomes a large number\n",
    "  \n",
    "How to turn F to be p-value?\n",
    "- Conceptually,\n",
    "- Generate a set of random data. \n",
    "- Calculate the mean and SS(mean), fit, SS(fit).\n",
    "- get F values from random dataset\n",
    " - now you have multiple F values\n",
    "- get back to original dataset and calculate F\n",
    " - the p-value is number of more extreme values of F divided by all the values of F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- https://matthew-brett.github.io/teaching/glm_intro.html\n",
    "- https://towardsdatascience.com/generalized-linear-models-9cbf848bb8ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other topics\n",
    "- logistic regression\n",
    " - weight\n",
    " - odds ratio\n",
    " - std error\n",
    " \n",
    "- scipy\n",
    " - stats.f_oneway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
